{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create clean database\n",
    "============\n",
    "\n",
    "From my raw database `raw_station` (table: `raw`), I can create a good database with a table for each station and a table that has the station information. I will be combining stations based on unit number, and *trying* to get a consistent time pattern for everything (00, 04, 08, 12, 16, 20h) to make comparison easier.\n",
    "\n",
    "I know that some stations are offset from that pattern by an hour, and there are additional measurements that fall in between regular updates. Additionally, I need to clean the data to get actual riderships and not counters, handle the large faulty numbers, and deal with negative ridership (counter counting down)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = 'mikemoran'\n",
    "raw_database = 'raw_stations'\n",
    "raw_table = 'raw'\n",
    "\n",
    "raw_engine = create_engine(f'postgres://{user}@localhost/{raw_database}')\n",
    "raw_conn = psycopg2.connect(database=raw_database, user=user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "units = '''\n",
    "    select distinct unit, station, linename, c_a from raw;\n",
    "'''\n",
    "\n",
    "station_details = pd.read_sql(units, raw_conn)\n",
    "station_details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "station_details.dropna(inplace=True)  # c_a is what I want...\n",
    "station_details[station_details.c_a == 'PTH16']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need the GPS coordinates of the station as well. Instead of worrying about the individual entrances and exits, we'll just use the general station coordinates from a separate table that I have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "known_stations = pd.read_csv('other_data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv', header=0)\n",
    "#                               usecols=['Division', 'Line',\n",
    "#                                        'Station Name', 'Station Latitude', 'Station Longitude',\n",
    "#                                        *[f'Route{n}' for n in range(1, 12)]])\n",
    "# known_stations.columns\n",
    "known_stations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "routes = [f'Route{n}' for n in range(1, 12)]\n",
    "known_stations.fillna('', inplace=True)\n",
    "\n",
    "known_stations['combined_routes'] = known_stations[routes].apply(\n",
    "    lambda x: ''.join(sorted([str(x[r]) for r in routes])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "known_stations.drop([*routes, 'Line'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_database = 'stations'\n",
    "station_info = 'station_info'\n",
    "\n",
    "info_engine = create_engine(f'postgres://{user}@localhost/{clean_database}')\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "\n",
    "# info_conn = psycopg2.connect(database=clean_database, user=user)\n",
    "station_details.to_sql(station_info, info_engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ca_unique = station_details.c_a.unique()\n",
    "ca_unique.shape, ca_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "    select * from raw where c_a = '{}';\n",
    "'''\n",
    "\n",
    "# for ca in ca_unique:\n",
    "#     print(ca)\n",
    "#     df = pd.read_sql(query.format(ca), raw_conn)\n",
    "#     df[['entries', 'exits']] = df.groupby(['unit', 'scp'])[['entries', 'exits']].diff()\n",
    "#     df.set_index('date_time', inplace=True)\n",
    "    \n",
    "ca = ca_unique[0]\n",
    "print(ca)\n",
    "df = pd.read_sql(query.format(ca), raw_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = df.set_index('date_time').sort_index()\n",
    "# df.reset_index(inplace=True, drop=True)\n",
    "# df.drop('index', inplace=True, axis=1)\n",
    "df.sort_values(by=['c_a', 'unit', 'scp', 'date_time'], inplace=True)\n",
    "df[['dentries', 'dexits']] = df.groupby(['unit', 'scp'])[['entries', 'exits']].diff()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = df.groupby('date_time').sum()\n",
    "# test.head()\n",
    "test_rs = test[['dentries', 'dexits']].resample('4H')\n",
    "test_result = test_rs.sum()\n",
    "test_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_result['dentries'].sum(), df['dentries'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query = '''\n",
    "    select * from raw where c_a = '{}';\n",
    "'''\n",
    "\n",
    "for ca in ca_unique:\n",
    "    print(ca)\n",
    "    df = pd.read_sql(query.format(ca), raw_conn)\n",
    "    df.sort_values(by=['c_a', 'unit', 'scp', 'date_time'], inplace=True)\n",
    "    df[['dentries', 'dexits']] = df.groupby(['unit', 'scp'])[['entries', 'exits']].diff()\n",
    "    df_ts = df.groupby('date_time').sum()\n",
    "    df_ts = df_ts[['dentries', 'dexits']].resample('4H').sum()\n",
    "    df_ts.columns = ['entries', 'exits']\n",
    "    df.to_sql(ca, info_engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The above takes approximately 24 hrs to run on my machine, since there aren't many data redution steps taken into account. Since the changepoints are what I'm interested in, the final database (if there is one) will be much smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
